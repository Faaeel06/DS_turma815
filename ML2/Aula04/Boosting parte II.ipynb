{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d71144",
   "metadata": {},
   "source": [
    "\n",
    "### Bagging vs Boosting\n",
    "\n",
    "Pra lembrar as principais diferenças entre os dois métodos de ensemble que estudamos:\n",
    "\n",
    "<img src=https://pluralsight2.imgix.net/guides/81232a78-2e99-4ccc-ba8e-8cd873625fdf_2.jpg width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d27787",
   "metadata": {},
   "source": [
    "## Gradient boosting\n",
    "\n",
    "Além dos métodos que estudamos, há ainda outras classes de métodos de ensemble!\n",
    "\n",
    "Em particular, a classe de modelos que se utilizam do procedimento de **gradient boosting**.\n",
    "\n",
    "O gradient boosting também é baseado no princípio de boosting (utilização de weak learners sequencialmente adicionados de modo a **sequencialmente minimizar os erros cometidos**).\n",
    "\n",
    "<img src=https://miro.medium.com/max/788/1*pEu2LNmxf9ttXHIALPcEBw.png width=600>\n",
    "\n",
    "Mas este método implementa o boosting através de um **gradiente** explícito.\n",
    "\n",
    "A ideia é que caminhemos na direção do **erro mínimo** de maneira iterativa **passo a passo**.\n",
    "\n",
    "Este caminho se dá justamente pelo **gradiente** da **função de custo/perda**, que mede justamente os erros cometidos.\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif width=400>\n",
    "\n",
    "Este método é conhecido como:\n",
    "\n",
    "### Gradiente descendente\n",
    "\n",
    "\n",
    "Deixei em ênfase porque este será um método de **enorme importância** no estudo de redes neurais (e é, em geral, um método de otimização muito utilizado).\n",
    "\n",
    "O objetivo geral do método é bem simples: determinar quais são os **parâmetros** da hipótese que minimizam a função de custo/perda. Para isso, o método \"percorre\" a função de erro, indo em direção ao seu mínimo (e este \"caminho\" feito na função se dá justamente pela **determinação iterativa dos parâmetros**, isto é, **a cada passo, chegamos mais perto dos parâmetros finais da hipótese**, conforme eles são ajustados aos dados.\n",
    "\n",
    "> **Pequeno interlúdio matemático:** o gradiente descendente implementado pelo gradient boosting é, na verdade, um **gradiente descendente funcional**, isto é, desejamos encontrar não um conjunto de parâmetros que minimiza o erro, mas sim **introduzir sequencialmente weak learners (hipótese simples) que minimizam o erro**. Desta forma, o gradient boosting minimiza a função de custo ao ecolher iterativamente hipóteses simples que apontam na direção do mínimo, neste espaço funcional.\n",
    "\n",
    "Apesar do interlúdio acima, não precisamos nos preocupar muito com os detalhes matemáticos: o que importa é entender que no caso do gradient boosting, há alguns pontos importantes:\n",
    "\n",
    "- Uma **função de custo/perda (loss)** é explicitamente minimizada por um procedimento de gradiente;\n",
    "\n",
    "- O gradiente está relacionado com o procedimento de **encadeamento progressivo entre weak learners**, seguindo a ideia do boosting.\n",
    "\n",
    "Pra quem quiser saber um pouco mais de detalhes (e se aventurar na matemática), sugiro [este post](https://www.gormanalysis.com/blog/gradient-boosting-explained/) ou então [este site](https://explained.ai/gradient-boosting/), que contém vários materiais ótimos para entender o método com todos os detalhes matemáticos.\n",
    "\n",
    "Os [vídeos do StatQuest](https://www.youtube.com/playlist?list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6) também são uma boa referência!\n",
    "\n",
    "\n",
    "\n",
    "As classes do sklearn são:\n",
    "\n",
    "- [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "\n",
    "- [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)\n",
    "\n",
    "E os principais hiperparâmetros a serem ajustados são:\n",
    "\n",
    "- `n_estimators` : novamente, o número de weak learners encadeados.\n",
    "\n",
    "- `learning_rate` : a constante que multiplica o gradiente no gradiente descendente. Essencialmente, controla o \"tamanho do passo\" a ser dado em direção ao mínimo.\n",
    "\n",
    "Segundo o próprio [User Guide](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting): \"*Empirical evidence suggests that small values of `learning_rate` favor better test error. The lireature recommends to set the learning rate to a small constant (e.g. `learning_rate <= 0.1`) and choose `n_estimators` by early stopping.*\"\n",
    "\n",
    "Ainda sobre a learning rate, as ilustrações a seguir ajudam a entender sua importância:\n",
    "\n",
    "<img src=https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png width=700>\n",
    "\n",
    "<img src=https://cdn-images-1.medium.com/max/1440/0*A351v9EkS6Ps2zIg.gif width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b9e4f",
   "metadata": {},
   "source": [
    "Vamos treinar nosso classificador baseline de gradient boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85f3dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ce997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2903c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f793cdc2",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Chegamos ao nosso último método de ensemble, o XGBoost (e**X**treme **G**radient **Boost**ing).\n",
    "\n",
    "Este método nada mais é que um gradient boosting, mas com algumas importantes modificações que lhe conferem o título de \"extreme\"! Em particular, duas alterações merecem destaque:\n",
    "\n",
    "- A adição de procedimentos de regularização (L1 e L2!), o que melhora consideravelmente sua capacidade de generalização;\n",
    "\n",
    "- A utilização de derivadas de segunda ordem (Hessiano) para o procedimento de gradiente.\n",
    "\n",
    "Para quem quiser se aventurar mais, sugiro algumas boas leituras:\n",
    "\n",
    "- [Este](https://shirinsplayground.netlify.app/2018/11/ml_basics_gbm/), explica bem as particularidades do XGBoost, além de dar uma boa introdução ao gradient boosting (o código é em R, então pode ignorar essa parte hehe);\n",
    "\n",
    "- [Este](https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb), introduz bem o método, enquanto enfativa suas particularidades, com alguns detalhes matemáticos;\n",
    "\n",
    "- [Este](https://xgboost.readthedocs.io/en/latest/tutorials/model.html), da própria documentação da biblioteca, traz uma explicação legal, e com alguns detalhes matemáticos;\n",
    "\n",
    "- [Este](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d), com uma discussão mais alto-nível (sem tantos detalhes) sobre o XGBoost e os motivos de seu sucesso.\n",
    "\n",
    "Infelizmente, o sklearn não tem o XGBoost implementado :(\n",
    "\n",
    "Mas, felizmente, existe uma biblioteca que o implementou, de maneira totalmente integrada ao sklearn!!\n",
    "\n",
    "A biblioteca é a [XGBoost](https://xgboost.readthedocs.io/en/latest/).\n",
    "\n",
    "Para instalar a biblioteca, o de sempre:\n",
    "\n",
    "`!pip install xgboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101cd317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
